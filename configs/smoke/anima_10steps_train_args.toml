anima_transformer = "/path/to/anima_transformer.safetensors"
pretrained_model_name_or_path = "/path/to/anima_transformer.safetensors"
vae = "/path/to/anima_vae.safetensors"
qwen = "/path/to/qwen_model_dir"
# LLMAdapter链路默认启用，T5 tokenizer 必填
t5_tokenizer_dir = "/path/to/t5_tokenizer_dir"

network_module = "networks.lokr_anima"
network_dim = 16
network_alpha = 16

output_dir = "./output/smoke"
output_name = "anima_smoke_10steps"
save_model_as = "safetensors"
save_every_n_steps = 10

dataset_config = "configs/smoke/anima_dataset.toml"
max_train_steps = 10
learning_rate = 0.0001
lr_scheduler = "constant"
lr_warmup_steps = 0

train_batch_size = 1
gradient_accumulation_steps = 1
mixed_precision = "bf16"
gradient_checkpointing = false
cache_latents = false
xformers = true
max_data_loader_n_workers = 0

optimizer_type = "AdamW8bit"
optimizer_args = ["weight_decay=0.01", "eps=1e-08", "betas=(0.9,0.95)"]

seed = 42
anima_seq_len = 128
anima_noise_offset = 0.0
