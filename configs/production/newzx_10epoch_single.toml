anima_transformer = "E:/sd-scripts-anima/anima-models/diffusion_models/anima-preview.safetensors"
pretrained_model_name_or_path = "E:/sd-scripts-anima/anima-models/diffusion_models/anima-preview.safetensors"
vae = "E:/sd-scripts-anima/anima-models/vae/qwen_image_vae.safetensors"
qwen = "E:/sd-scripts-anima/anima-models/text_encoders"
t5_tokenizer_dir = "E:/sd-scripts-anima/anima-models/t5/t5_old"
auto_download_t5_tokenizer = true
t5_tokenizer_repo_id = "https://huggingface.co/nvidia/Cosmos-Predict2-2B-Text2Image/tree/main/tokenizer"
t5_tokenizer_subfolder = "tokenizer"
t5_tokenizer_modelscope_fallback = true
t5_tokenizer_modelscope_repo_id = "nv-community/Cosmos-Predict2-2B-Text2Image"
t5_tokenizer_modelscope_revision = "master"
t5_tokenizer_modelscope_subfolder = "tokenizer"

network_module = "networks.lokr_anima"
network_dim = 100000
network_alpha = 16

output_dir = "E:/sd-scripts-anima/output/newzx_e10_bs8"
output_name = "newzx_lokr_e10_bs8_lr1e4"
save_model_as = "safetensors"
save_every_n_epochs = 1

max_train_epochs = 10
learning_rate = 0.0001
lr_scheduler = "constant"
lr_warmup_steps = 0

train_batch_size = 8
gradient_accumulation_steps = 4
mixed_precision = "bf16"
gradient_checkpointing = true
cache_latents = true
xformers = true
max_data_loader_n_workers = 2
persistent_data_loader_workers = true

optimizer_type = "AdamW8bit"
optimizer_args = ["weight_decay=0.01", "eps=1e-08", "betas=(0.9,0.95)"]

train_data_dir = "E:/lora-scripts-v1.10.0/train/newzx"
caption_extension = ".txt"
shuffle_caption = true
keep_tokens = 0
flip_aug = false
resolution = "512"
enable_bucket = true
min_bucket_reso = 256
max_bucket_reso = 1024
bucket_reso_steps = 64

seed = 42
anima_seq_len = 128
anima_noise_offset = 0.0
